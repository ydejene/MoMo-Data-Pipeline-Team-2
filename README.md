# MoMo-Data-Pipeline-Team-2

## Team 2 - MoMo SMS ETL Project
### Team Members
- Brian
- Derick
- Habibllah
- Yonas

---

## Links
- **Architecture Diagram:** [Link to image or file](https://drive.google.com/file/d/1_R2R9Gr6-Rh1-_83e8GfeI4qMhwHSEkr/view?usp=sharing)
- **Scrum Board:** [Team 2 Scrum Board](https://alustudent-team-k1plq8kl.atlassian.net/jira/software/projects/T2MSEP/boards/34?jql=&atlOrigin=eyJpIjoiN2ZkZGMzNjFhMTZkNGQzODg4MTM1YzI2ZGIyZDZiODAiLCJwIjoiaiJ9)

---

## Project Overview
An enterprise-level fullstack application to process MoMo SMS data (XML), categorize transactions, and visualize insights via a dashboard.

---

## Table of Contents (more to be added as we build up)

- [Project Structure](#project-structure)
- [System Architecture](#system-architecture)

---

## System Architecture

### High-Level Overview
Our system follows a layered architecture pattern with clear separation of concerns:
- **Data Ingestion** â†’ **Processing** â†’ **Storage** â†’ **API/Export** â†’ **Visualization**

[Link to Architecture Diagram](./docs/architecture-diagram.png)

### Architecture Components

#### 1ï¸ Data Source Layer
| Component | Purpose | Technology |
|-----------|---------|------------|
| `momo.xml` | Raw MoMo SMS transaction data in XML format | XML |

#### 2ï¸ ETL Pipeline (Backend Processing)
| Component | File | Purpose |
|-----------|------|---------|
| **XML Parser** | `etl/parse_xml.py` | Reads and validates XML structure, extracts transaction records |
| **Data Cleaner** | `etl/clean_normalize.py` | Normalizes amounts (removes currency symbols), standardizes date formats, normalizes phone numbers |
| **Categorizer** | `etl/categorize.py` | Applies business rules to classify transactions (e.g., airtime, transfer, bill payment) |
| **Error Handler** | Built into pipeline | Captures malformed/invalid data and routes to dead letter queue |
| **ETL Orchestrator** | `etl/run.py` | CLI tool that coordinates the entire ETL workflow |
| **Logger** | `data/logs/etl.log` | Records all ETL operations, errors, and audit trail for debugging and monitoring |

#### 3ï¸ Storage Layer
| Component | File/Location | Purpose |
|-----------|---------------|---------|
| **SQLite Database** | `data/db.sqlite3` | Relational database storing normalized transaction data in structured tables |
| **Dashboard Cache** | `data/processed/dashboard.json` | Pre-aggregated analytics data for fast frontend loading without database queries |
| **Dead Letter Queue** | `data/logs/dead_letter/` | Stores unparsed or invalid XML snippets for manual review and data quality monitoring |

#### 4ï¸ API Layer (Optional - Bonus Feature)
| Component | File | Purpose |
|-----------|------|---------|
| **FastAPI Backend** | `api/app.py` | RESTful API server providing programmatic access to transaction data |
| **Endpoints** | | â€¢ `GET /transactions` - Retrieve filtered transaction list<br>â€¢ `GET /analytics` - Get aggregated statistics<br>â€¢ `GET /categories` - List transaction categories |
| **Database Helper** | `api/db.py` | SQLite connection pool and query utilities |
| **Data Schemas** | `api/schemas.py` | Pydantic models for request/response validation |

#### 5ï¸ Presentation Layer (Frontend)
| Component | File | Purpose |
|-----------|------|---------|
| **Dashboard UI** | `index.html` | Main user interface for viewing transactions and analytics |
| **Visualization Engine** | `web/chart_handler.js` | Fetches data and renders interactive charts and tables |
| **Styling** | `web/styles.css` | Responsive design and visual styling |
| **Assets** | `web/assets/` | Icons, images, and other static resources |

#### ğŸ”§ Automation Scripts
| Script | Purpose | Usage |
|--------|---------|-------|
| `scripts/run_etl.sh` | Executes the complete ETL pipeline from XML to database | `./scripts/run_etl.sh` |
| `scripts/export_json.sh` | Regenerates `dashboard.json` with latest aggregated data | `./scripts/export_json.sh` |
| `scripts/serve_frontend.sh` | Starts local development server for testing the dashboard | `./scripts/serve_frontend.sh` |

### Data Flow
```
1. XML File (momo.xml) 
   â†“
2. ETL Pipeline
   â€¢ Parse XML â†’ Extract transactions
   â€¢ Clean & Normalize â†’ Standardize formats
   â€¢ Categorize â†’ Apply business rules
   â€¢ Validate â†’ Route errors to dead letter
   â†“
3. Dual Storage
   â€¢ Valid data â†’ SQLite Database (normalized tables)
   â€¢ Invalid data â†’ Dead Letter Queue (for review)
   â€¢ Logs â†’ etl.log (audit trail)
   â†“
4. Data Access Layer
   â€¢ Option A: Export aggregated data â†’ dashboard.json
   â€¢ Option B: API endpoints â†’ Real-time queries
   â†“
5. Frontend Visualization
   â€¢ Load data (from JSON or API)
   â€¢ Render charts, tables, analytics
   â€¢ Display to end user
```

### Design Decisions

**Why ETL Pattern?**
- Separates concerns: parsing, cleaning, and loading are independent
- Enables error handling at each stage
- Makes testing and debugging easier

**Why SQLite?**
- Lightweight, no separate server required
- Perfect for development and small-to-medium datasets
- Easy to backup (single file)

**Why dashboard.json?**
- Reduces database load for frequently accessed aggregations
- Faster frontend loading (no API/DB overhead)
- Can be version controlled for change tracking

**Why Dead Letter Queue?**
- Preserves problematic data for investigation
- Prevents data loss
- Helps identify data quality issues

### Error Handling Strategy

Our system implements comprehensive error handling:
- **Validation errors** â†’ Logged with details in `etl.log`
- **Malformed XML** â†’ Stored in `dead_letter/` for manual review
- **Processing failures** â†’ Transaction rolled back, error logged
- **Database errors** â†’ Graceful degradation, user notification

### Scalability Considerations

Current architecture supports:
- Thousands of transactions per run
- Multiple ETL executions per day
- Concurrent frontend users (via static files or API)

For enterprise-scale needs:
- Migrate SQLite â†’ PostgreSQL/MySQL
- Add message queue (RabbitMQ/Kafka) for real-time processing
- Implement caching layer (Redis)
- Deploy API with load balancing

---

## Project Structure
```
â”œâ”€â”€ README.md                         # Setup, run, overview
â”œâ”€â”€ .env.example                      # DATABASE_URL or path to SQLite
â”œâ”€â”€ requirements.txt                  # lxml/ElementTree, dateutil, (FastAPI optional)
â”œâ”€â”€ index.html                        # Dashboard entry (static)
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ styles.css                    # Dashboard styling
â”‚   â”œâ”€â”€ chart_handler.js              # Fetch + render charts/tables
â”‚   â””â”€â”€ assets/                       # Images/icons (optional)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                          # Provided XML input (git-ignored)
â”‚   â”‚   â””â”€â”€ momo.xml
â”‚   â”œâ”€â”€ processed/                    # Cleaned/derived outputs for frontend
â”‚   â”‚   â””â”€â”€ dashboard.json            # Aggregates the dashboard reads
â”‚   â”œâ”€â”€ db.sqlite3                    # SQLite DB file
â”‚   â””â”€â”€ logs/
â”‚       â”œâ”€â”€ etl.log                   # Structured ETL logs
â”‚       â””â”€â”€ dead_letter/              # Unparsed/ignored XML snippets
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                     # File paths, thresholds, categories
â”‚   â”œâ”€â”€ parse_xml.py                  # XML parsing (ElementTree/lxml)
â”‚   â”œâ”€â”€ clean_normalize.py            # Amounts, dates, phone normalization
â”‚   â”œâ”€â”€ categorize.py                 # Simple rules for transaction types
â”‚   â”œâ”€â”€ load_db.py                    # Create tables + upsert to SQLite
â”‚   â””â”€â”€ run.py                        # CLI: parse -> clean -> categorize -> load -> export JSON
â”œâ”€â”€ api/                              # Optional (bonus)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py                        # Minimal FastAPI with /transactions, /analytics
â”‚   â”œâ”€â”€ db.py                         # SQLite connection helpers
â”‚   â””â”€â”€ schemas.py                    # Pydantic response models
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_etl.sh                    # python etl/run.py --xml data/raw/momo.xml
â”‚   â”œâ”€â”€ export_json.sh                # Rebuild data/processed/dashboard.json
â”‚   â””â”€â”€ serve_frontend.sh             # python -m http.server 8000 (or Flask static)
â””â”€â”€ tests/
    â”œâ”€â”€ test_parse_xml.py             # Small unit tests
    â”œâ”€â”€ test_clean_normalize.py
    â””â”€â”€ test_categorize.py
```
---

## References & Credits

### Tools Used
- **Eraser.io** - Architecture diagram design and generation
- **GitHub** - Version control and team collaboration
- **Jira** - Agile project management and sprint tracking

### Documentation
- Project structure inspired by industry best practices
- ETL patterns based on data engineering principles
